{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bea13b-67bd-4a0e-8eab-3b8ffd37259e",
   "metadata": {},
   "source": [
    "# BasicTick: Create Everything\n",
    "This notebook will use the AWS boto3 APIs to create the needed resources for the basic tick application.\n",
    "\n",
    "## AWS Resources Created\n",
    "- Database   \n",
    "- Changeset to add data to database   \n",
    "- Scaling Group that will contain all clusters   \n",
    "- Shared Volume   \n",
    "- Dataview of database on the shared volume   \n",
    "- Clusters: TP, HDB, Gateway, and RDB   \n",
    "\n",
    "### Non AWS\n",
    "- Feedhandler (run locally) to push data to TP   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c6572a-0972-4867-8c02-ea2c6c98427a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5f1d4a-ed45-44e3-bf75-9bdb75fcddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pykx as kx\n",
    "\n",
    "from managed_kx import *\n",
    "from env import *\n",
    "\n",
    "# Cluster names and database\n",
    "from basictick_setup import *\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Source data directory\n",
    "SOURCE_DATA_DIR=\"hdb\"\n",
    "\n",
    "# Code directory\n",
    "CODEBASE=\"basictick\"\n",
    "\n",
    "# S3 Destinations\n",
    "S3_CODE_PATH=\"code\"\n",
    "S3_DATA_PATH=\"data\"\n",
    "\n",
    "NODE_TYPE=\"kx.sg.4xlarge\"\n",
    "\n",
    "DATABASE_CONFIG=[{ \n",
    "    'databaseName': DB_NAME,\n",
    "    'dataviewName': DBVIEW_NAME\n",
    "    }]\n",
    "CODE_CONFIG={ 's3Bucket': S3_BUCKET, 's3Key': f'{S3_CODE_PATH}/{CODEBASE}.zip' }\n",
    "\n",
    "NAS1_CONFIG= {\n",
    "        'type': 'SSD_250',\n",
    "        'size': 1200\n",
    "}\n",
    "\n",
    "RDB_INIT_SCRIPT='rdbmkdb.q'\n",
    "RDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'dbname', 'value': DB_NAME}, \n",
    "    { 'key': 'tp', 'value': TP_CLUSTER_NAME }, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "TP_INIT_SCRIPT='tpmkdb.q'\n",
    "TP_CMD_ARGS=[\n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "HDB_INIT_SCRIPT='hdbmkdb.q'\n",
    "HDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'dbname', 'value': DB_NAME}, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "GW_INIT_SCRIPT='gwmkdb.q'\n",
    "GW_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'rdb_name', 'value': RDB_CLUSTER_NAME}, \n",
    "    { 'key': 'hdb_name', 'value': HDB_CLUSTER_NAME}, \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfe7d89-9f5d-4ceb-ac8c-1f5054a6f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using variables ...\n"
     ]
    }
   ],
   "source": [
    "# triggers credential get\n",
    "session=None\n",
    "\n",
    "try:\n",
    "    # aws: use ada for credentials\n",
    "    os.system([\"which\", \"ada\"])\n",
    "    os.system(f\"ada credentials update --account={ACCOUNT_ID} --provider=isengard --role=Admin --once\")\n",
    "except: \n",
    "    None\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is None:\n",
    "    print(\"Using Defaults ...\")\n",
    "    # create AWS session: using access variables\n",
    "    session = boto3.Session()\n",
    "else:\n",
    "    print(\"Using variables ...\")\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        aws_session_token=AWS_SESSION_TOKEN\n",
    "    )\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace', endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d4047-9583-4b09-b75d-98fd2ddd6c36",
   "metadata": {},
   "source": [
    "# Create the Database\n",
    "Create a database from the supplied data in hdb.tar.gz.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8efd4-a881-4871-af85-9b6125457d06",
   "metadata": {},
   "source": [
    "## Untar HDB Data in hdb.tar.gz\n",
    "Data will be found in hdb directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fab7b4-284a-472b-8089-01fa1db8ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf hdb.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef94785-4b95-468d-a078-c1698996e8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 12 ec2-user ec2-user   245 Apr 24  2023 .\n",
      "drwxrwxr-x  6 ec2-user ec2-user  4096 Feb 13 16:02 ..\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.14\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.15\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.16\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.17\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.18\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.19\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.20\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.21\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.22\n",
      "drwxr-xr-x  3 ec2-user ec2-user    29 Apr 24  2023 2023.04.23\n",
      "-rw-r--r--  1 ec2-user ec2-user 16392 Apr 24  2023 sym\n"
     ]
    }
   ],
   "source": [
    "!ls -la hdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf690f2-c465-4df8-90f5-1e3b808bb368",
   "metadata": {},
   "source": [
    "## Stage HDB Data on S3\n",
    "Using AWS cli, copy hdb to staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca7f0d7-32cd-443b-b642-e7209b8516ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 2023.04.14/\n",
      "                           PRE 2023.04.15/\n",
      "                           PRE 2023.04.16/\n",
      "                           PRE 2023.04.17/\n",
      "                           PRE 2023.04.18/\n",
      "                           PRE 2023.04.19/\n",
      "                           PRE 2023.04.20/\n",
      "                           PRE 2023.04.21/\n",
      "                           PRE 2023.04.22/\n",
      "                           PRE 2023.04.23/\n",
      "2023-10-18 17:14:47      16392 sym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_DEST=f\"s3://{S3_BUCKET}/{S3_DATA_PATH}/{SOURCE_DATA_DIR}/\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 sync  --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST}\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 sync  --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST}\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c759c4-ee6c-45c5-a9f6-6acacea3a3be",
   "metadata": {},
   "source": [
    "## Create Managed Database\n",
    "Using the AWS APIs, create a managed database in Managed kdb Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55bd8d3-5629-46f9-bc1f-47bb0308dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING Database: basictickdb\n",
      "CREATED Database: basictickdb\n",
      "{\n",
      "    \"createdTimestamp\": \"2024-02-13 16:04:27.175000+00:00\",\n",
      "    \"databaseArn\": \"arn:aws:finspace:us-east-1:829845998889:kxEnvironment/jlcenjvtkgzrdek2qqv7ic/kxDatabase/basictickdb\",\n",
      "    \"databaseName\": \"basictickdb\",\n",
      "    \"description\": \"Basictick kdb database\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-02-13 16:04:27.175000+00:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# assume it exists\n",
    "create_db=False\n",
    "\n",
    "try:\n",
    "    resp = client.get_kx_database(environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "except:\n",
    "    # does not exist, will create\n",
    "    create_db=True\n",
    "\n",
    "if create_db:\n",
    "    print(f\"CREATING Database: {DB_NAME}\")\n",
    "    resp = client.create_kx_database(environmentId=ENV_ID, databaseName=DB_NAME, description=\"Basictick kdb database\")\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "\n",
    "    print(f\"CREATED Database: {DB_NAME}\")\n",
    "\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1194e-0c04-49a3-a7e7-a1d23fcff0d9",
   "metadata": {},
   "source": [
    "## Add HDB Data to Database\n",
    "Add the data in the local hdb directory to the managed database using the changeset mechanism. The Data will be copied to S3 then ingested with the create-kx-changeset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae61f04-1c9c-468e-bb38-b2e0b94897a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changeset...\n",
      "{\n",
      "    \"changeRequests\": [\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.23/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.23/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.15/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.15/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.14/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.14/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.22/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.22/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.18/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.18/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.20/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.20/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.16/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.16/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.17/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.17/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.21/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.21/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2023.04.19/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2023.04.19/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/sym\"\n",
      "        }\n",
      "    ],\n",
      "    \"changesetId\": \"GMbRmzL61rRX1jXmU2zGlg\",\n",
      "    \"createdTimestamp\": \"2024-02-13 16:04:27.768000+00:00\",\n",
      "    \"databaseName\": \"basictickdb\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-02-13 16:04:27.768000+00:00\",\n",
      "    \"status\": \"PENDING\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "changes=[]\n",
    "\n",
    "for f in os.listdir(f\"{SOURCE_DATA_DIR}\"):\n",
    "    if os.path.isdir(f\"{SOURCE_DATA_DIR}/{f}\"):\n",
    "        changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}/\", 'dbPath': f\"/{f}/\" } )\n",
    "    else:\n",
    "        changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}\", 'dbPath': f\"/\" } )\n",
    "        \n",
    "resp = client.create_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, \n",
    "    changeRequests=changes)\n",
    "\n",
    "resp.pop('ResponseMetadata', None)\n",
    "changeset_id = resp['changesetId']\n",
    "\n",
    "print(\"Changeset...\")\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4422bdd-7d44-4fb0-8018-0bebd6987704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is IN_PROGRESS, total wait 0:00:00, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:10, waiting 10 sec ...\n",
      "**Done**\n"
     ]
    }
   ],
   "source": [
    "wait_for_changeset_status(client, environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "print(\"**Done**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba008f3-4991-474c-9b3e-43a1dca56257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Database: basictickdb, Changesets: 1 \n",
      "====================================================================================================\n",
      "  Changeset: GMbRmzL61rRX1jXmU2zGlg: Created: 2024-02-13 16:04:27.768000+00:00 (COMPLETED)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5032d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5032d_level0_col0\" class=\"col_heading level0 col0\" >changeType</th>\n",
       "      <th id=\"T_5032d_level0_col1\" class=\"col_heading level0 col1\" >s3Path</th>\n",
       "      <th id=\"T_5032d_level0_col2\" class=\"col_heading level0 col2\" >dbPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row0_col0\" class=\"data row0 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row0_col1\" class=\"data row0 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.23/</td>\n",
       "      <td id=\"T_5032d_row0_col2\" class=\"data row0 col2\" >/2023.04.23/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row1_col0\" class=\"data row1 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row1_col1\" class=\"data row1 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.15/</td>\n",
       "      <td id=\"T_5032d_row1_col2\" class=\"data row1 col2\" >/2023.04.15/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row2_col0\" class=\"data row2 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row2_col1\" class=\"data row2 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.14/</td>\n",
       "      <td id=\"T_5032d_row2_col2\" class=\"data row2 col2\" >/2023.04.14/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row3_col0\" class=\"data row3 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row3_col1\" class=\"data row3 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.22/</td>\n",
       "      <td id=\"T_5032d_row3_col2\" class=\"data row3 col2\" >/2023.04.22/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row4_col0\" class=\"data row4 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row4_col1\" class=\"data row4 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.18/</td>\n",
       "      <td id=\"T_5032d_row4_col2\" class=\"data row4 col2\" >/2023.04.18/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row5_col0\" class=\"data row5 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row5_col1\" class=\"data row5 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.20/</td>\n",
       "      <td id=\"T_5032d_row5_col2\" class=\"data row5 col2\" >/2023.04.20/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row6_col0\" class=\"data row6 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row6_col1\" class=\"data row6 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.16/</td>\n",
       "      <td id=\"T_5032d_row6_col2\" class=\"data row6 col2\" >/2023.04.16/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row7_col0\" class=\"data row7 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row7_col1\" class=\"data row7 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.17/</td>\n",
       "      <td id=\"T_5032d_row7_col2\" class=\"data row7 col2\" >/2023.04.17/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row8_col0\" class=\"data row8 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row8_col1\" class=\"data row8 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.21/</td>\n",
       "      <td id=\"T_5032d_row8_col2\" class=\"data row8 col2\" >/2023.04.21/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row9_col0\" class=\"data row9 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row9_col1\" class=\"data row9 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2023.04.19/</td>\n",
       "      <td id=\"T_5032d_row9_col2\" class=\"data row9 col2\" >/2023.04.19/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5032d_row10_col0\" class=\"data row10 col0\" >PUT</td>\n",
       "      <td id=\"T_5032d_row10_col1\" class=\"data row10 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/sym</td>\n",
       "      <td id=\"T_5032d_row10_col2\" class=\"data row10 col2\" >/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0dfbea2040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "note_str = \"\"\n",
    "\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    note_str = \"<<Could not get changesets>>\"\n",
    "    \n",
    "print(100*\"=\")\n",
    "print(f\"Database: {DB_NAME}, Changesets: {len(c_set_list)} {note_str}\")\n",
    "print(100*\"=\")\n",
    "\n",
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "for c in c_set_list:\n",
    "    c_set_id = c['changesetId']\n",
    "    print(f\"  Changeset: {c_set_id}: Created: {c['createdTimestamp']} ({c['status']})\")\n",
    "    c_rqs = client.get_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, changesetId=c_set_id)['changeRequests']\n",
    "\n",
    "    chs_pdf = pd.DataFrame.from_dict(c_rqs).style.hide(axis='index')\n",
    "    display(chs_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae0232-3666-491f-8891-dae30e12c9d8",
   "metadata": {},
   "source": [
    "# Create Scaling Group\n",
    "The scaling group represents the total compute avilable to the application. All clusters will be placed into the scaling group ans share the compute and memory of the scaling group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613be7f8-fb82-4415-b30c-186ed470dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_scaling_group(\n",
    "    environmentId = ENV_ID, \n",
    "    scalingGroupName = SCALING_GROUP_NAME,\n",
    "    hostType=NODE_TYPE,\n",
    "    availabilityZoneId = AZ_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab8d287-4c24-4e53-8dbd-642c73c7faf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '056438c6-9a08-423d-bafc-de227f027697',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '245',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:04:51 GMT',\n",
       "   'x-amzn-requestid': '056438c6-9a08-423d-bafc-de227f027697',\n",
       "   'x-amz-apigw-id': 'TFPtUGx5oAMEGTw=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb9321-78b3a3f80bbc8eef198171e3',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': 'qo0LdQR8UVy9bg0Mnink-YBq3A5-xCMPcKYN-zhd-QDoGGGq4EceBw=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'environmentId': 'jlcenjvtkgzrdek2qqv7ic',\n",
       " 'scalingGroupName': 'SCALING_GROUP_basictickdb',\n",
       " 'hostType': 'kx.sg.4xlarge',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'status': 'CREATING',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 2, 13, 16, 4, 50, 803000, tzinfo=tzlocal()),\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 4, 50, 774000, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943fb16-8989-4199-a0fd-5c7c0d5aa56e",
   "metadata": {},
   "source": [
    "# Create Shared Volume\n",
    "The shared volume is a common storage device for the application. Every cluster using the shared volume will have a writable directory named after the cluster, can read the directories named after other clusters in the application using the volume. Also, there is a common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a8a247-d029-4f9b-aaf5-c6e2ffe200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_volume(\n",
    "    environmentId = ENV_ID, \n",
    "    volumeType = 'NAS_1',\n",
    "    volumeName = VOLUME_NAME,\n",
    "    description = 'Shared volume between TP and RDB',\n",
    "    nas1Configuration = NAS1_CONFIG,\n",
    "    azMode='SINGLE',\n",
    "    availabilityZoneIds=[ AZ_ID ]    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec4876f-5ecc-420e-aff8-9c88c8c9deb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6e2fd5f2-4334-49c2-bce6-9de7155ece56',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '435',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:04:52 GMT',\n",
       "   'x-amzn-requestid': '6e2fd5f2-4334-49c2-bce6-9de7155ece56',\n",
       "   'x-amz-apigw-id': 'TFPtkEvNIAMEZFA=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb9323-67e0fd8155d27a9850420e6b',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': 'JP8G3ADX1HJ9IIDJlsUigbhKgcbRi_y3yqfr2NkzQV32F_0AbzB59Q=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'environmentId': 'jlcenjvtkgzrdek2qqv7ic',\n",
       " 'volumeName': 'RDB_TP_SHARED',\n",
       " 'volumeType': 'NAS_1',\n",
       " 'volumeArn': 'arn:aws:finspace:us-east-1:829845998889:kxEnvironment/jlcenjvtkgzrdek2qqv7ic/kxVolume/RDB_TP_SHARED',\n",
       " 'nas1Configuration': {'type': 'SSD_250', 'size': 1200},\n",
       " 'status': 'CREATING',\n",
       " 'azMode': 'SINGLE',\n",
       " 'description': 'Shared volume between TP and RDB',\n",
       " 'availabilityZoneIds': ['use1-az6'],\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 4, 52, 281000, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e718537-8853-4f21-8cc7-36b489e380c4",
   "metadata": {},
   "source": [
    "# Wait for Volume and Scaling Group\n",
    "Before proceeding to use Volumes and Scaling groups, wait for their creation to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ed8931-e458-4ffb-83cc-0aa4da4d9f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Scaling Group: SCALING_GROUP_basictickdb status is now ACTIVE, total wait 0:05:30\n",
      "** DONE **\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is now ACTIVE, total wait 0:02:30\n",
      "** DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the scaling group to create\n",
    "wait_for_scaling_group_status(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME, show_wait=True)\n",
    "print(\"** DONE **\")\n",
    "\n",
    "# wait for the volume to create\n",
    "wait_for_volume_status(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41eaeb-9c8e-44d3-b8bc-f354142f9140",
   "metadata": {},
   "source": [
    "# Create Dataview\n",
    "Create a dataview, for a specific (static) version of the database and have all of its data cached using the shared volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03434316-4ccc-420d-adee-715e6eb1bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "resp = client.create_kx_dataview(\n",
    "    environmentId = ENV_ID, \n",
    "    databaseName=DB_NAME, \n",
    "    dataviewName=DBVIEW_NAME,\n",
    "    azMode='SINGLE',\n",
    "    availabilityZoneId=AZ_ID,\n",
    "    changesetId=c_set_list[-1]['changesetId'],\n",
    "    segmentConfigurations=[\n",
    "        { \n",
    "            'dbPaths': ['/*'],\n",
    "            'volumeName': VOLUME_NAME\n",
    "        }\n",
    "    ],\n",
    "    autoUpdate=False,\n",
    "    description = f'Dataview of database'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734ab7da-ef78-4701-9a58-1eeacbd9c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is now ACTIVE, total wait 0:06:30\n",
      "** DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the view to create\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea431b0-c501-46bb-b72a-a5eb80a335b0",
   "metadata": {},
   "source": [
    "# Create Clusters\n",
    "With foundation resources now completed, create the needed clusters for the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c06ab-4dcb-4cc6-abc9-2c77ff3a4242",
   "metadata": {},
   "source": [
    "## Stage Code to S3\n",
    "Code to be used in this application must be staged to an S3 bucket the service can read from, that code will then be deployed to the clusters as part of their creation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b502a0a5-8610-4fc8-b6b7-04c47e89ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: connectmkdb.q (deflated 63%)\n",
      "  adding: example.schema.q (deflated 12%)\n",
      "  adding: feedmkdb.q (deflated 53%)\n",
      "  adding: funcDownHandle.q (deflated 33%)\n",
      "  adding: gwmkdbcluster.q (deflated 61%)\n",
      "  adding: hdbmkdb.q (deflated 44%)\n",
      "  adding: loadDep.q (deflated 58%)\n",
      "  adding: query.q (deflated 19%)\n",
      "  adding: rdbmkdb.q (deflated 55%)\n",
      "  adding: tp.q (deflated 52%)\n",
      "  adding: tpmkdb.q (deflated 52%)\n",
      "  adding: gwmkdb.q (deflated 61%)\n",
      "  adding: aws.q (deflated 73%)\n",
      "upload: ./basictick.zip to s3://kdb-demo-829845998889-kms/code/basictick.zip\n",
      "2023-06-05 21:25:21          0 \n",
      "2024-02-13 16:19:33      13775 basictick.zip\n",
      "2024-01-09 20:42:41        542 code.zip\n",
      "2023-12-21 19:47:37        574 codebundle.zip\n",
      "2024-02-02 21:34:56        582 codebundle1.zip\n",
      "2023-12-21 21:26:00        582 codebundle2.zip\n",
      "2023-11-22 14:58:53       1530 jpmc_code.zip\n",
      "2024-01-01 19:57:08      33781 kdb-tick-flat-largetable.zip\n",
      "2023-12-30 22:56:33      38867 kdb-tick-flat.zip\n",
      "2024-01-08 13:05:33      28741 kdb-tick.zip\n",
      "2023-08-22 16:58:18        765 qcode.zip\n",
      "2023-10-23 18:38:07       1390 taqcode.zip\n",
      "2024-01-30 16:52:19       3583 tradeplus.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the code\n",
    "os.system(f\"cd {CODEBASE}; zip -r -X ../{CODEBASE}.zip . -x '*.ipynb_checkpoints*';\")\n",
    "\n",
    "# copy code to S3\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 cp  --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "aws s3 ls s3://{S3_BUCKET}/code/\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 cp  --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "aws s3 ls s3://{S3_BUCKET}/code/\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88716ead-2205-4971-a5b0-33ffe96d7f85",
   "metadata": {},
   "source": [
    "## Create Tickerplant (TP) Cluster\n",
    "Tickerplant will deliver data from feedhandler to subscribing RDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28bf38a1-7733-4eb2-839a-a302a57c8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_cluster(\n",
    "    environmentId=ENV_ID, \n",
    "    clusterName=TP_CLUSTER_NAME,\n",
    "    clusterType='TICKERPLANT',\n",
    "    releaseLabel = '1.0',\n",
    "    executionRole=EXECUTION_ROLE,\n",
    "    scalingGroupConfiguration={\n",
    "#        'cpu': 1,\n",
    "#        'memoryLimit': 6,\n",
    "        'memoryReservation': 6,\n",
    "        'nodeCount': 1,\n",
    "        'scalingGroupName': SCALING_GROUP_NAME,\n",
    "    },\n",
    "#    savedownStorageConfiguration ={ 'volumeName': VOLUME_NAME },\n",
    "    tickerplantLogConfiguration ={ 'tickerplantLogVolumes': [ VOLUME_NAME ] },\n",
    "    clusterDescription=\"Created with create_all notebook\",\n",
    "    code=CODE_CONFIG,\n",
    "    initializationScript=TP_INIT_SCRIPT,\n",
    "    commandLineArguments=TP_CMD_ARGS,\n",
    "    azMode=AZ_MODE,\n",
    "    availabilityZoneId=AZ_ID,\n",
    "    vpcConfiguration={ \n",
    "        'vpcId': VPC_ID,\n",
    "        'securityGroupIds': SECURITY_GROUPS,\n",
    "        'subnetIds': SUBNET_IDS,\n",
    "        'ipAddressType': 'IP_V4' }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fcb19c5-0579-4f04-b4a2-e393cdd7e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '53cc4b44-70ea-43be-83eb-127613cb7f73',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '1179',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:19:38 GMT',\n",
       "   'x-amzn-requestid': '53cc4b44-70ea-43be-83eb-127613cb7f73',\n",
       "   'x-amz-apigw-id': 'TFR3UGiGoAMEc2g=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb9694-581924657a93fb96117a19fd',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': '2Suq522Z8ILXJh-GBObBsk9vRUD8t4v05IvYhEGd6A02f1nkKHdREw=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'status': 'PENDING',\n",
       " 'clusterName': 'TP_basictickdb',\n",
       " 'clusterType': 'TICKERPLANT',\n",
       " 'tickerplantLogConfiguration': {'tickerplantLogVolumes': ['RDB_TP_SHARED']},\n",
       " 'volumes': [{'volumeName': 'RDB_TP_SHARED', 'volumeType': 'NAS_1'}],\n",
       " 'clusterDescription': 'Created with create_all notebook',\n",
       " 'releaseLabel': '1.0',\n",
       " 'vpcConfiguration': {'vpcId': 'vpc-0fe2b9c50f3ad382f',\n",
       "  'securityGroupIds': ['sg-0c99f1cfb9c3c7fd9'],\n",
       "  'subnetIds': ['subnet-04052219ec25b062b'],\n",
       "  'ipAddressType': 'IP_V4'},\n",
       " 'initializationScript': 'tpmkdb.q',\n",
       " 'commandLineArguments': [{'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}],\n",
       " 'code': {'s3Bucket': 'kdb-demo-829845998889-kms',\n",
       "  's3Key': 'code/basictick.zip'},\n",
       " 'executionRole': 'arn:aws:iam::829845998889:role/kdb-all-user',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 37, 983000, tzinfo=tzlocal()),\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 37, 947000, tzinfo=tzlocal()),\n",
       " 'scalingGroupConfiguration': {'scalingGroupName': 'SCALING_GROUP_basictickdb',\n",
       "  'memoryReservation': 6,\n",
       "  'nodeCount': 1}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a31ee-07e1-408b-8fe9-2a9fb92f4df2",
   "metadata": {},
   "source": [
    "## Create historical Database (HDB) Cluster\n",
    "A 3 node HDB cluster will serve up queries for T+1 and older data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3abd68fa-5690-4374-bb68-4277bb87cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_cluster(\n",
    "    environmentId=ENV_ID, \n",
    "    clusterName=HDB_CLUSTER_NAME,\n",
    "    clusterType='HDB',\n",
    "    releaseLabel = '1.0',\n",
    "    executionRole=EXECUTION_ROLE,\n",
    "    databases=DATABASE_CONFIG,\n",
    "    scalingGroupConfiguration={\n",
    "#        'cpu': 1,\n",
    "#        'memoryLimit': 6,\n",
    "        'memoryReservation': 6,\n",
    "        'nodeCount': 3,\n",
    "        'scalingGroupName': SCALING_GROUP_NAME,\n",
    "    },\n",
    "    clusterDescription=\"Created with create_all notebook\",\n",
    "    code=CODE_CONFIG,\n",
    "    initializationScript=HDB_INIT_SCRIPT,\n",
    "    commandLineArguments=HDB_CMD_ARGS,\n",
    "    azMode=AZ_MODE,\n",
    "    availabilityZoneId=AZ_ID,\n",
    "    vpcConfiguration={ \n",
    "        'vpcId': VPC_ID,\n",
    "        'securityGroupIds': SECURITY_GROUPS,\n",
    "        'subnetIds': SUBNET_IDS,\n",
    "        'ipAddressType': 'IP_V4' }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e6db32-ec28-4496-947d-7154462eca01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f5a2f194-a2d7-4a53-8cc8-a382badaea59',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '1542',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:19:41 GMT',\n",
       "   'x-amzn-requestid': 'f5a2f194-a2d7-4a53-8cc8-a382badaea59',\n",
       "   'x-amz-apigw-id': 'TFR4MGCHIAMEAoQ=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb969a-5f7cf4f8116d848761f09918',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': 'FSiMIxb9cmx8ox8_QqF7EG9AUbmFvoFOTL42pQaKERPdhOgnLWOlzw=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'status': 'PENDING',\n",
       " 'clusterName': 'HDB_basictickdb',\n",
       " 'clusterType': 'HDB',\n",
       " 'volumes': [{'volumeName': 'RDB_TP_SHARED', 'volumeType': 'NAS_1'}],\n",
       " 'databases': [{'databaseName': 'basictickdb',\n",
       "   'cacheConfigurations': [],\n",
       "   'dataviewConfiguration': {'dataviewName': 'basictickdb_DBVIEW',\n",
       "    'dataviewVersionId': 'rMbRnxcYFi2AS9YbQngi8w',\n",
       "    'changesetId': 'GMbRmzL61rRX1jXmU2zGlg',\n",
       "    'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "      'volumeName': 'RDB_TP_SHARED'}]}}],\n",
       " 'clusterDescription': 'Created with create_all notebook',\n",
       " 'releaseLabel': '1.0',\n",
       " 'vpcConfiguration': {'vpcId': 'vpc-0fe2b9c50f3ad382f',\n",
       "  'securityGroupIds': ['sg-0c99f1cfb9c3c7fd9'],\n",
       "  'subnetIds': ['subnet-04052219ec25b062b'],\n",
       "  'ipAddressType': 'IP_V4'},\n",
       " 'initializationScript': 'hdbmkdb.q',\n",
       " 'commandLineArguments': [{'key': 's', 'value': '2'},\n",
       "  {'key': 'dbname', 'value': 'basictickdb'},\n",
       "  {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}],\n",
       " 'code': {'s3Bucket': 'kdb-demo-829845998889-kms',\n",
       "  's3Key': 'code/basictick.zip'},\n",
       " 'executionRole': 'arn:aws:iam::829845998889:role/kdb-all-user',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 41, 476000, tzinfo=tzlocal()),\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 41, 460000, tzinfo=tzlocal()),\n",
       " 'scalingGroupConfiguration': {'scalingGroupName': 'SCALING_GROUP_basictickdb',\n",
       "  'memoryReservation': 6,\n",
       "  'nodeCount': 3}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a21ff-a51f-47d3-9b70-c4eb863fa34f",
   "metadata": {},
   "source": [
    "## Create Gateway\n",
    "The Gateway will handle client queries for data in the RDB and HDB. Gateways act as single API access points for data queries and query both the RDB and HDB for data and aggregate results back to requestor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f27b7a1-4e8a-4b32-9175-a83ffb97f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_cluster(\n",
    "    environmentId=ENV_ID, \n",
    "    clusterName=GW_CLUSTER_NAME,\n",
    "    clusterType='GATEWAY',\n",
    "    releaseLabel = '1.0',\n",
    "    scalingGroupConfiguration={\n",
    "#        'cpu': 2,\n",
    "#        'memoryLimit': 6,\n",
    "        'memoryReservation': 6,\n",
    "        'nodeCount': 1,\n",
    "        'scalingGroupName': SCALING_GROUP_NAME,\n",
    "    },\n",
    "#    savedownStorageConfiguration ={ 'volumeName': VOLUME_NAME },\n",
    "    clusterDescription=\"Created with create_all notebook\",\n",
    "    executionRole=EXECUTION_ROLE,\n",
    "    code=CODE_CONFIG,\n",
    "    initializationScript=GW_INIT_SCRIPT,\n",
    "    commandLineArguments=GW_CMD_ARGS,\n",
    "    azMode=AZ_MODE,\n",
    "    availabilityZoneId=AZ_ID,\n",
    "    vpcConfiguration={ \n",
    "        'vpcId': VPC_ID,\n",
    "        'securityGroupIds': SECURITY_GROUPS,\n",
    "        'subnetIds': SUBNET_IDS,\n",
    "        'ipAddressType': 'IP_V4' }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a150561-bac3-4c69-9d9a-f316256d900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8ad41437-c89b-4209-9391-d5829ea0140e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '1161',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:19:44 GMT',\n",
       "   'x-amzn-requestid': '8ad41437-c89b-4209-9391-d5829ea0140e',\n",
       "   'x-amz-apigw-id': 'TFR4rFMdIAMEPMQ=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb969d-157daa614e5986e70a579574',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': 'BY3yUPUfWvE8IxmQynTnI6U6VTTeesTjFYbe2eWZLrXYwSblWhyvWw=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'status': 'PENDING',\n",
       " 'clusterName': 'GATEWAY_basictickdb',\n",
       " 'clusterType': 'GATEWAY',\n",
       " 'volumes': [],\n",
       " 'clusterDescription': 'Created with create_all notebook',\n",
       " 'releaseLabel': '1.0',\n",
       " 'vpcConfiguration': {'vpcId': 'vpc-0fe2b9c50f3ad382f',\n",
       "  'securityGroupIds': ['sg-0c99f1cfb9c3c7fd9'],\n",
       "  'subnetIds': ['subnet-04052219ec25b062b'],\n",
       "  'ipAddressType': 'IP_V4'},\n",
       " 'initializationScript': 'gwmkdb.q',\n",
       " 'commandLineArguments': [{'key': 's', 'value': '2'},\n",
       "  {'key': 'rdb_name', 'value': 'RDB_basictickdb'},\n",
       "  {'key': 'hdb_name', 'value': 'HDB_basictickdb'}],\n",
       " 'code': {'s3Bucket': 'kdb-demo-829845998889-kms',\n",
       "  's3Key': 'code/basictick.zip'},\n",
       " 'executionRole': 'arn:aws:iam::829845998889:role/kdb-all-user',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 43, 823000, tzinfo=tzlocal()),\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 19, 43, 813000, tzinfo=tzlocal()),\n",
       " 'scalingGroupConfiguration': {'scalingGroupName': 'SCALING_GROUP_basictickdb',\n",
       "  'memoryReservation': 6,\n",
       "  'nodeCount': 1}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1688e-850d-423a-b98f-8cc67eb9ca8d",
   "metadata": {},
   "source": [
    "## Create Realtime Database (RDB)\n",
    "The RDB will subscribe to the tickerplant and capture real time data published by the tickerplant (as published by the feedhandler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec822269-6c16-4ae9-9116-fdb4a048682b",
   "metadata": {},
   "source": [
    "### Wait for TP before creating RDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8efea149-22e9-4503-9c31-903b742a77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: TP_basictickdb status is PENDING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:12:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:13:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:13:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:14:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:14:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:15:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:15:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is now RUNNING, total wait 0:16:00\n",
      "TP is running\n"
     ]
    }
   ],
   "source": [
    "wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=TP_CLUSTER_NAME, show_wait=True)\n",
    "print(\"TP is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e39ebf3-6940-40f1-a7f8-90efb3846f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_kx_cluster(\n",
    "    environmentId=ENV_ID, \n",
    "    clusterName=RDB_CLUSTER_NAME,\n",
    "    clusterType='RDB',\n",
    "    releaseLabel = '1.0',\n",
    "    executionRole=EXECUTION_ROLE,\n",
    "    databases=DATABASE_CONFIG,\n",
    "    scalingGroupConfiguration={\n",
    "#        'cpu': 1,\n",
    "#        'memoryLimit': 6,\n",
    "        'memoryReservation': 6,\n",
    "        'nodeCount': 1,\n",
    "        'scalingGroupName': SCALING_GROUP_NAME,\n",
    "    },\n",
    "    savedownStorageConfiguration ={ 'volumeName': VOLUME_NAME },\n",
    "    clusterDescription=\"Created with create_all notebook\",\n",
    "    code=CODE_CONFIG,\n",
    "    initializationScript=RDB_INIT_SCRIPT,\n",
    "    commandLineArguments=RDB_CMD_ARGS,\n",
    "    azMode=AZ_MODE,\n",
    "    availabilityZoneId=AZ_ID,\n",
    "    vpcConfiguration={ \n",
    "        'vpcId': VPC_ID,\n",
    "        'securityGroupIds': SECURITY_GROUPS,\n",
    "        'subnetIds': SUBNET_IDS,\n",
    "        'ipAddressType': 'IP_V4' }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f09cb24b-5d6f-455a-9b45-cc9a8733b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2e375d13-18f6-4e58-aebe-d855f08731c8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'content-length': '1631',\n",
       "   'connection': 'keep-alive',\n",
       "   'date': 'Tue, 13 Feb 2024 16:35:55 GMT',\n",
       "   'x-amzn-requestid': '2e375d13-18f6-4e58-aebe-d855f08731c8',\n",
       "   'x-amz-apigw-id': 'TFUQRHARIAMEf4g=',\n",
       "   'x-amzn-trace-id': 'Root=1-65cb9a67-799aa87a7a3e6ca7357c968c',\n",
       "   'x-cache': 'Miss from cloudfront',\n",
       "   'via': '1.1 d5710f445906ae917df909d01c495c9e.cloudfront.net (CloudFront)',\n",
       "   'x-amz-cf-pop': 'IAD50-C2',\n",
       "   'x-amz-cf-id': 'nrFg1CxHQ7NdDm5AiFD3muCy9z2iD3PVtduqyU1xfPb89D6aAv800g=='},\n",
       "  'RetryAttempts': 0},\n",
       " 'status': 'PENDING',\n",
       " 'clusterName': 'RDB_basictickdb',\n",
       " 'clusterType': 'RDB',\n",
       " 'volumes': [{'volumeName': 'RDB_TP_SHARED', 'volumeType': 'NAS_1'}],\n",
       " 'databases': [{'databaseName': 'basictickdb',\n",
       "   'cacheConfigurations': [],\n",
       "   'dataviewConfiguration': {'dataviewName': 'basictickdb_DBVIEW',\n",
       "    'dataviewVersionId': 'rMbRnxcYFi2AS9YbQngi8w',\n",
       "    'changesetId': 'GMbRmzL61rRX1jXmU2zGlg',\n",
       "    'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "      'volumeName': 'RDB_TP_SHARED'}]}}],\n",
       " 'clusterDescription': 'Created with create_all notebook',\n",
       " 'releaseLabel': '1.0',\n",
       " 'vpcConfiguration': {'vpcId': 'vpc-0fe2b9c50f3ad382f',\n",
       "  'securityGroupIds': ['sg-0c99f1cfb9c3c7fd9'],\n",
       "  'subnetIds': ['subnet-04052219ec25b062b'],\n",
       "  'ipAddressType': 'IP_V4'},\n",
       " 'initializationScript': 'rdbmkdb.q',\n",
       " 'commandLineArguments': [{'key': 's', 'value': '2'},\n",
       "  {'key': 'dbname', 'value': 'basictickdb'},\n",
       "  {'key': 'tp', 'value': 'TP_basictickdb'},\n",
       "  {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}],\n",
       " 'code': {'s3Bucket': 'kdb-demo-829845998889-kms',\n",
       "  's3Key': 'code/basictick.zip'},\n",
       " 'executionRole': 'arn:aws:iam::829845998889:role/kdb-all-user',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 2, 13, 16, 35, 55, 284000, tzinfo=tzlocal()),\n",
       " 'savedownStorageConfiguration': {'volumeName': 'RDB_TP_SHARED'},\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'createdTimestamp': datetime.datetime(2024, 2, 13, 16, 35, 55, 246000, tzinfo=tzlocal()),\n",
       " 'scalingGroupConfiguration': {'scalingGroupName': 'SCALING_GROUP_basictickdb',\n",
       "  'memoryReservation': 6,\n",
       "  'nodeCount': 1}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491391-8c01-4190-a2bd-23fa888bf781",
   "metadata": {},
   "source": [
    "## Wait for all clusters to finish creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0946ca26-c4b0-410d-ade5-18a47bf2318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: TP_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "Cluster: RDB_basictickdb status is PENDING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:12:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is now RUNNING, total wait 0:13:00\n",
      "Cluster: HDB_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "Cluster: GATEWAY_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "** ALL DONE **\n"
     ]
    }
   ],
   "source": [
    "# Wait for all clusters to start\n",
    "for c in all_clusters.values():\n",
    "    wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=c, show_wait=True)\n",
    "\n",
    "print(\"** ALL DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230207e8-c297-4d7e-af65-4396fa5b4deb",
   "metadata": {},
   "source": [
    "# List Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c50c578-05e8-49e7-8deb-1f6b94b10221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusterName</th>\n",
       "      <th>status</th>\n",
       "      <th>clusterType</th>\n",
       "      <th>capacityConfiguration</th>\n",
       "      <th>commandLineArguments</th>\n",
       "      <th>clusterDescription</th>\n",
       "      <th>lastModifiedTimestamp</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>databaseName</th>\n",
       "      <th>cacheConfigurations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GATEWAY_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>GATEWAY</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'rdb_name', 'value': 'RDB_basictickdb'}, {'key': 'hdb_name', 'value': 'HDB_basictickdb'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-02-13 16:36:33.229000+00:00</td>\n",
       "      <td>2024-02-13 16:19:43.813000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HDB_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>HDB</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'dbname', 'value': 'basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-02-13 16:36:24.568000+00:00</td>\n",
       "      <td>2024-02-13 16:19:41.460000+00:00</td>\n",
       "      <td>basictickdb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RDB_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>RDB</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'dbname', 'value': 'basictickdb'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-02-13 16:49:01.534000+00:00</td>\n",
       "      <td>2024-02-13 16:35:55.246000+00:00</td>\n",
       "      <td>basictickdb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TP_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>TICKERPLANT</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-02-13 16:35:24.743000+00:00</td>\n",
       "      <td>2024-02-13 16:19:37.947000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            clusterName   status  clusterType capacityConfiguration  \\\n",
       "4   GATEWAY_basictickdb  RUNNING      GATEWAY                  None   \n",
       "5       HDB_basictickdb  RUNNING          HDB                  None   \n",
       "9       RDB_basictickdb  RUNNING          RDB                  None   \n",
       "10       TP_basictickdb  RUNNING  TICKERPLANT                  None   \n",
       "\n",
       "                                                                                                                                                commandLineArguments  \\\n",
       "4                                     [{'key': 's', 'value': '2'}, {'key': 'rdb_name', 'value': 'RDB_basictickdb'}, {'key': 'hdb_name', 'value': 'HDB_basictickdb'}]   \n",
       "5                                             [{'key': 's', 'value': '2'}, {'key': 'dbname', 'value': 'basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]   \n",
       "9   [{'key': 's', 'value': '2'}, {'key': 'dbname', 'value': 'basictickdb'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]   \n",
       "10                                                                                                                   [{'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]   \n",
       "\n",
       "                  clusterDescription            lastModifiedTimestamp  \\\n",
       "4   Created with create_all notebook 2024-02-13 16:36:33.229000+00:00   \n",
       "5   Created with create_all notebook 2024-02-13 16:36:24.568000+00:00   \n",
       "9   Created with create_all notebook 2024-02-13 16:49:01.534000+00:00   \n",
       "10  Created with create_all notebook 2024-02-13 16:35:24.743000+00:00   \n",
       "\n",
       "                   createdTimestamp databaseName cacheConfigurations  \n",
       "4  2024-02-13 16:19:43.813000+00:00         None                 NaN  \n",
       "5  2024-02-13 16:19:41.460000+00:00  basictickdb                None  \n",
       "9  2024-02-13 16:35:55.246000+00:00  basictickdb                None  \n",
       "10 2024-02-13 16:19:37.947000+00:00         None                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdf = get_clusters(client, environmentId=ENV_ID)\n",
    "\n",
    "if cdf is not None:\n",
    "    cdf = cdf[cdf['clusterName'].isin(all_clusters.values())]\n",
    "\n",
    "display(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cb064-adfc-4f57-8104-a730184197c3",
   "metadata": {},
   "source": [
    "# Start FeedHandler\n",
    "All infrastructure is now running, You can start a feedhandler to send data into the running tickerplant (TP).\n",
    "\n",
    "\n",
    "## From the console\n",
    "```\n",
    "$ TP_CONN=\"<connection string to cluster>\"\n",
    "$ cd basictick\n",
    "$ q feedmkdb.q -p 5030 -tp $TP_CONN\n",
    "```\n",
    "\n",
    "Here we use Python to get the connection string, set environment variables, and run the feedhandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccfe94d9-206c-40cb-b6a5-332d489872c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"connected to tp\"\n",
      "\"connected to tp\"\n"
     ]
    }
   ],
   "source": [
    "# get the connection string\n",
    "conn_str = get_kx_connection_string(client, environmentId=ENV_ID, clusterName=TP_CLUSTER_NAME, userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "# populate the environment variable with connection string\n",
    "os.putenv(\"CONN_STR\", conn_str)\n",
    "os.putenv(\"FH_PORT\", f\"{FH_PORT}\")\n",
    "\n",
    "# start q process feedmkdb to connect to the TP at $TP_CONN\n",
    "subprocess.Popen(f\"cd {CODEBASE}; nohup q feedmkdb.q -p $FH_PORT -tp $CONN_STR\", shell=True)\n",
    "\n",
    "# wait for feedhandler to start doing its thing\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "588fa530-58e1-4f58-a699-2f99b2e31f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>process</th>\n",
       "      <th>connected</th>\n",
       "      <th>handle</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tp</td>\n",
       "      <td>1b</td>\n",
       "      <td>5i</td>\n",
       "      <td>:tcps://vpce-03780042a641200b8-0y7p4ih5.vpce-svc-0be765235870753a5.us-east-1.vpce.amazonaws.com:443:bob:Host=vpce-03780042a641200b8-0y7p4ih5.vpce-svc-0be765235870753a5.us-east-1.vpce.amazonaws.com&amp;Port=443&amp;User=bob&amp;Action=finspace%3AConnectKxCluster&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDNHqDzDk3vs9Bv01G%2Bj0IgYwAHG8fOSXRtayUJTZ11jAIgWAoxGCGWV0bTPTYx7M3o7Qxn4muwky8FPfW5l1YhHH0q9wIIehAAGgw4Mjk4NDU5OTg4ODkiDOoo2Z3tNDAz%2FOKW4irUAr8JHJ591G%2BXMNpbAUyGEj9oNV9CrhbYG5%2BIAwvujFjcai0NcYnuMo9djbqFyLscv%2BCOxDHTX3pd%2FkLj9muuS6kbTNoyrhXVDPshQ9mGqqUkGVdpnvFbxay4Ky%2F3QckgwA4NHUm5i4yXGYpSX5Gfw6rFOLkhTs0JPfNFhxoFBGYayBMawDz6b2t0X9C4aIe6LxOkhZb7xns%2FMznk02xkDSmkANSM9AK%2B1FNm8qhHM7xDcJmQUVyhC84koVTnPngnI81jdGHIKKOh0EyhVd3YVlf%2FSjFDHw%2BjocotuDntWYzHV4ImIQeUo4IhuVh4qX0YszHe4XDY7e6WI0gt45j0VMG4JL1zV5P0lnx0kLTkSbpBo0v90PDeoCO8yW5G1tea09o0tNUFd3JBnw6u%2FKGCkgq0UmKZqf4ViooWXtDiFiG1WvHQgDcHjotlXYXqwJqyKqqkMmUwgruurgY6vwGOnE4FBbZRs5diqIhBPdCj7TvnHdSldSnO8JUEDW2lPw0GACZSxfgmu9Y%2Bu2Jvc490gkjkKmKK3w60x3j1C9NJhMPPiiadia0zw11eRknsD97nv5pQSBPmfd7jiyeJtWpXu%2BSjAFC7YLmXfd2dIJ%2Fqn4vb7WYGi5Ju7V7%2B7ekdkRxTkmyH3RiTVaZXg4seZQeM3qcwO1tKwV0LtdpsMP%2FdOMN9tZ%2F0rQZqFLC2hVVcaPHent8rZ%2BataVep7qWSpg%3D%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240213T164906Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=900&amp;X-Amz-Credential=ASIA4CNVNBUU476M7IM4%2F20240213%2Fus-east-1%2Ffinspace-apricot%2Faws4_request&amp;X-Amz-Signature=a115b2ab31acc1e57d570bf11a26f54b6e80316ba5cab6535862be8dfec636e8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Table(pykx.q('\n",
       "process connected handle address                                             ..\n",
       "-----------------------------------------------------------------------------..\n",
       "tp      1         5      :tcps://vpce-03780042a641200b8-0y7p4ih5.vpce-svc-0be..\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check Feedhandler connections, should show connected to the \n",
    "fh=kx.QConnection(port=FH_PORT)\n",
    "\n",
    "display( fh (\"select process,connected,handle,address from .conn.procs\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a23b-b100-4763-9c50-c819f5824202",
   "metadata": {},
   "source": [
    "# All Processes Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f33240-bb12-49f3-8d9c-5783c25eb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2024-02-13 16:49:08.091687\n"
     ]
    }
   ],
   "source": [
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05fa3-fbe0-443b-986e-428fb1ca4ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
