{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outside FinSpace Cluster Management\n",
    "Clusters can be managed form outside of FinSpace as well as this notebook will demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd \n",
    "import boto3\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ---------------------------------------------------\n",
    "#### Identify your region\n",
    "#### ---------------------------------------------------\n",
    "region_name  = 'us-east-1'\n",
    "\n",
    "### ----------------------------------------------------------------\n",
    "### Get Credentials from the \"API Credentials\" in FinSpace \n",
    "### ----------------------------------------------------------------\n",
    "hab_access_key_id     = ''\n",
    "hab_secret_access_key = ''\n",
    "hab_session_token     = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Helper\n",
    "This class wraps the service API so its easier to consume those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load ../Utilities/finspace.py\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from botocore.config import Config\n",
    "from boto3.session import Session\n",
    "\n",
    "# Base FinSpace class\n",
    "class FinSpace:\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        config = Config(retries = {'max_attempts': 0, 'mode': 'standard'}),\n",
    "        boto_session: Session = None,\n",
    "        dev_overrides: dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To configure this class object, simply instantiate with no-arg if hitting prod endpoint, or else override it: \n",
    "        e.g.\n",
    "           `hab = FinSpaceAnalyticsManager(region_name = 'us-east-1', \n",
    "           dev_overrides = {'hfs_endpoint': 'https://39g32x40jk.execute-api.us-east-1.amazonaws.com/alpha'})`\n",
    "        \"\"\"\n",
    "        self.hfs_endpoint = None\n",
    "        self.region_name  = None\n",
    "        \n",
    "        if dev_overrides != None:\n",
    "            if 'hfs_endpoint' in dev_overrides:\n",
    "                 self.hfs_endpoint = dev_overrides['hfs_endpoint']\n",
    "\n",
    "            if 'region_name' in dev_overrides:\n",
    "                self.region_name = dev_overrides['region_name']\n",
    "        else:\n",
    "            self.region_name = self.get_region_name() \n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self._boto3_session = boto3.session.Session(region_name = self.region_name) if boto_session is None else  boto_session\n",
    "        \n",
    "        print(f\"endpoint: {self.hfs_endpoint}\")\n",
    "        print(f\"region_name: {self.region_name}\")\n",
    "                \n",
    "        self.client = self._boto3_session.client('finspace-data', endpoint_url = self.hfs_endpoint, config = self.config )\n",
    "\n",
    "    def get_region_name(self):\n",
    "        req = urllib.request.Request(\"http://169.254.169.254/latest/meta-data/placement/region\")\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            return response.read().decode(\"utf-8\")\n",
    "\n",
    "    #--------------------------------------\n",
    "    # Utility Functions\n",
    "    #--------------------------------------\n",
    "    def get_list(self, all_list: dir, name: str):\n",
    "        \"\"\"\n",
    "        Search for name found in the all_list dir and return that list of things. \n",
    "        Removes repetitive code found in functions that call boto apis then search for the expected returned items\n",
    "        \n",
    "        :param all_list: list of things to search\n",
    "        :type: dir:\n",
    "        \n",
    "        :param name: name to search for in all_lists\n",
    "        :type: str\n",
    "        \n",
    "        :return: list of items found in name\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        \n",
    "        # is the given name found, is found, add to list\n",
    "        if name in all_list:\n",
    "            for s in all_list[name]:\n",
    "                r.append(s)\n",
    "        \n",
    "        # return the list\n",
    "        return r\n",
    "        \n",
    "    #--------------------------------------\n",
    "    # Classification Functions\n",
    "    #--------------------------------------\n",
    "        \n",
    "    def list_classifications(self):\n",
    "        \"\"\"\n",
    "        Return list of all classifications\n",
    "\n",
    "        :return: all classifications\n",
    "        \"\"\"\n",
    "        all_list = self.client.list_classifications(sort='NAME')\n",
    " \n",
    "        return ( self.get_list(all_list, 'classifications') )\n",
    "\n",
    "    def classification_names(self):\n",
    "        \"\"\" \n",
    "        Get the classifications names\n",
    "\n",
    "        :return list of classifications names only\n",
    "        \"\"\"\n",
    "        classification_names = []\n",
    "        all_classifications = self.list_classifications()\n",
    "        for c in all_classifications :\n",
    "            classification_names.append(c['name'])\n",
    "        return classification_names\n",
    "    \n",
    "    def classification(self, name: str):\n",
    "        \"\"\" \n",
    "        Exact name search for a classification of the given name\n",
    "\n",
    "        :param name: name of the classification to find\n",
    "        :type: str\n",
    "\n",
    "        :return \n",
    "        \"\"\"\n",
    "        \n",
    "        all_classifications = self.list_classifications()\n",
    "        existing_classification = next((c for c in all_classifications if c['name'] == name), None)\n",
    "        if existing_classification:\n",
    "            return existing_classification\n",
    "    \n",
    "    def describe_classification(self, classification_id: str):\n",
    "        \"\"\"\n",
    "        Calls the describe classification API function and only returns the taxonomy portion of the response.\n",
    "        \n",
    "        :param classification_id: the GUID of the classification to get description of\n",
    "        :type: str\n",
    "        \"\"\"\n",
    "        resp = None\n",
    "        taxonomy_details_resp = self.client.describe_taxonomy(taxonomyId=classification_id)\n",
    "        \n",
    "        if 'taxonomy' in taxonomy_details_resp:\n",
    "            resp = taxonomy_details_resp['taxonomy']\n",
    "        \n",
    "        return( resp )\n",
    "   \n",
    "    def create_classification( self, classification_definition):\n",
    "        resp = self.client.create_taxonomy(taxonomyDefinition = classification_definition)\n",
    "        \n",
    "        taxonomy_id = resp[\"taxonomyId\"]\n",
    "        \n",
    "        return(taxonomy_id)\n",
    "\n",
    "    def delete_classification( self, classification_id): \n",
    "        resp = self.client.delete_taxonomy(taxonomyId = classification_id)\n",
    "        \n",
    "        if (resp['ResponseMetadata']['HTTPStatusCode'] != 200):\n",
    "            return resp\n",
    "        \n",
    "        return True\n",
    "    #--------------------------------------\n",
    "    # Attribute Set Functions\n",
    "    #--------------------------------------\n",
    "        \n",
    "    def list_attribute_sets(self):\n",
    "        \"\"\"\n",
    "        Get list of all dataset_types in the system\n",
    "        \n",
    "        :return: list of dataset types\n",
    "        \"\"\"\n",
    "        resp = self.client.list_dataset_types()\n",
    "        results = resp['datasetTypeSummaries']\n",
    "\n",
    "        while \"nextToken\" in resp:\n",
    "            resp = self.client.list_dataset_types(nextToken=resp['nextToken'])\n",
    "            results.extend(resp['datasetTypeSummaries'])\n",
    "\n",
    "        return( results )\n",
    "\n",
    "    def attribute_set_names(self):\n",
    "        \"\"\" \n",
    "        Get the list of all dataset type names\n",
    "\n",
    "        :return list of all dataset type names\n",
    "        \"\"\"\n",
    "\n",
    "        dataset_type_names = []\n",
    "        all_dataset_types = self.list_dataset_types()\n",
    "        for c in all_dataset_types :\n",
    "            dataset_type_names.append(c['name'])\n",
    "        return dataset_type_names\n",
    "    \n",
    "    def attribute_set(self, name: str):\n",
    "        \"\"\" \n",
    "        Exact name search for a dataset type of the given name\n",
    "\n",
    "        :param name: name of the dataset type to find\n",
    "        :type: str\n",
    "\n",
    "        :return \n",
    "        \"\"\"\n",
    "        \n",
    "        all_dataset_types = self.list_dataset_types()\n",
    "        existing_dataset_type = next((c for c in all_dataset_types if c['name'] == name), None)\n",
    "        if existing_dataset_type:\n",
    "            return existing_dataset_type\n",
    "    \n",
    "    def describe_attribute_set(self, attribute_set_id: str):\n",
    "        \"\"\"\n",
    "        Calls the describe dataset type API function and only returns the dataset type portion of the response.\n",
    "        \n",
    "        :param dataset_type_id: the GUID of the dataset type to get description of\n",
    "        :type: str\n",
    "        \"\"\"\n",
    "        resp = None\n",
    "        dataset_type_details_resp = self.client.describe_dataset_type(datasetTypeId=attribute_set_id)\n",
    "        \n",
    "        if 'datasetType' in dataset_type_details_resp:\n",
    "            resp = dataset_type_details_resp['datasetType']\n",
    "            \n",
    "        return(resp)\n",
    "\n",
    "    def create_attribute_set( self, attribute_set_def):\n",
    "        resp = self.client.create_dataset_type(datasetTypeDefinition = attribute_set_def)\n",
    "        \n",
    "        att_id = resp[\"datasetTypeId\"]\n",
    "        \n",
    "        return(att_id)\n",
    "\n",
    "    def delete_attribute_set( self, attribute_set_id: str): \n",
    "        resp = self.client.delete_attribute_set(attributeSetId = attribute_set_id)\n",
    "        \n",
    "        if (resp['ResponseMetadata']['HTTPStatusCode'] != 200):\n",
    "            return resp\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    #--------------------------------------\n",
    "    # Permission Group Functions\n",
    "    #--------------------------------------\n",
    "\n",
    "    def list_permission_groups(self, maxResults: int):\n",
    "        all_perms = self.client.list_permission_groups(MaxResults = maxResults)\n",
    "        return( self.get_list( all_perms, 'permissionGroups') )\n",
    "    \n",
    "    def permission_group( self, name):\n",
    "        all_groups = self.list_permission_groups(maxResults=100)\n",
    "\n",
    "        existing_group = next((c for c in all_groups if c['name'] == name), None)\n",
    "        \n",
    "        if existing_group:\n",
    "            return existing_group\n",
    "\n",
    "    def describe_permission_group( self, permission_group_id: str ):\n",
    "        resp = None\n",
    "        \n",
    "        perm_resp = self.client.describe_permission_group( permissionGroupId = permission_group_id )\n",
    "        \n",
    "        if 'permissionGroup' in perm_resp:\n",
    "            resp = perm_resp['permissionGroup']\n",
    "            \n",
    "        return(resp)\n",
    "    \n",
    "    #--------------------------------------\n",
    "    # Dataset Functions\n",
    "    #--------------------------------------\n",
    "        \n",
    "    def describe_dataset_details(self, dataset_id: str):\n",
    "        \"\"\"\n",
    "        Calls the describe dataset details API function and only returns the dataset details portion of the response.\n",
    "        \n",
    "        :param dataset_id: the GUID of the dataset to get description of\n",
    "        :type: str\n",
    "        \"\"\"\n",
    "        resp = None\n",
    "        dataset_details_resp = self.client.describe_dataset_details(datasetId=dataset_id)\n",
    "        \n",
    "        if 'dataset' in dataset_details_resp:\n",
    "            resp = dataset_details_resp[\"dataset\"]\n",
    "        \n",
    "        return( resp )\n",
    "    \n",
    "    def create_dataset(self, name: str, description: str, permission_group_id: str, dataset_permissions: [], kind: str, owner_info, schema):\n",
    "        \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Warning, dataset names are not unique, be sure to check for the same name dataset before creating a new one\n",
    "        \n",
    "        :param name: Name of the dataset\n",
    "        :type: str\n",
    "\n",
    "        :param description: Description of the dataset\n",
    "        :type: str\n",
    "\n",
    "        :param permission_group_id: permission group for the dataset\n",
    "        :type: str\n",
    "\n",
    "        :param dataset_permissions: permissions for the group on the dataset\n",
    "\n",
    "        :param kind: Kind of dataset, choices: TABULAR\n",
    "        :type: str\n",
    "\n",
    "        :param owner_info: owner information for the dataset\n",
    "\n",
    "        :param schema: Schema of the dataset\n",
    "\n",
    "        :return: the dataset_id of the created dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        if dataset_permissions: \n",
    "            requestDatasetPermissions = [{\"permission\": permissionName} for permissionName in dataset_permissions]\n",
    "        else:\n",
    "            requestDatasetPermissions = []\n",
    "            \n",
    "        response = self.client.create_dataset(name=name,\n",
    "                                     permissionGroupId=permission_group_id,\n",
    "                                     datasetPermissions=requestDatasetPermissions,\n",
    "                                     kind=kind,\n",
    "                                     description=description.replace('\\n',' '),\n",
    "                                     ownerInfo=owner_info,\n",
    "                                     schema=schema)\n",
    "        \n",
    "        return response[\"datasetId\"]\n",
    "    \n",
    "    def ingest_from_s3(self, \n",
    "        s3_location: str, \n",
    "        dataset_id: str, \n",
    "        change_type: str, \n",
    "        wait_for_completion=True,\n",
    "        format_type = \"CSV\", \n",
    "        format_params = {'separator':',', 'withHeader':'true'} ):\n",
    "        \"\"\"\n",
    "        Creates a changeset and ingests the data given in the S3 location into the changeset\n",
    "        \n",
    "        :param s3_location: the source location of the data for the changeset, will be copied into the changeset\n",
    "        :stype: str\n",
    "        \n",
    "        :param dataset_id: the identifier of the containing dataset for the changeset to be created for this data\n",
    "        :type: str\n",
    "        \n",
    "        :param change_type: What is the kind of changetype?  \"APPEND\", \"REPLACE\" are the choices\n",
    "        :type: str\n",
    "        \n",
    "        :param wait_for_completion: Boolean, should the function wait for the operation to complete?\n",
    "        :type: str\n",
    "        \n",
    "        :return: the id of the changeset created\n",
    "        \"\"\"\n",
    "        create_changeset_response = self.client.create_changeset(\n",
    "            datasetId=dataset_id, \n",
    "            changeType=change_type,\n",
    "            sourceType='S3',\n",
    "            sourceParams={'s3SourcePath': s3_location},\n",
    "            formatType=format_type.upper(),\n",
    "            formatParams=format_params\n",
    "        )\n",
    "\n",
    "        changeset_id = create_changeset_response['changeset']['id']\n",
    "        \n",
    "        if wait_for_completion:\n",
    "            self.wait_for_ingestion(dataset_id, changeset_id)\n",
    "        return changeset_id\n",
    "\n",
    "    def describe_changeset(self, dataset_id: str, changeset_id: str):\n",
    "        \"\"\"\n",
    "        Function to get a description of the the givn changeset for the given dataset\n",
    "        \n",
    "        :param dataset_id: identifier of the dataset\n",
    "        :type: str\n",
    "        \n",
    "        :param changeset_id: the idenfitier of the changeset\n",
    "        :type: str\n",
    "        \n",
    "        :return: all information about the changeset, if found\n",
    "        \"\"\"\n",
    "        describe_changeset_resp = self.client.describe_changeset(datasetId=dataset_id, id=changeset_id)\n",
    "        \n",
    "        return describe_changeset_resp['changeset']\n",
    "\n",
    "    def create_as_of_view(self, dataset_id: str, as_of_date: datetime, destination_type: str, \n",
    "                              partition_columns=[], sort_columns=[], destination_properties={}, wait_for_completion=True):\n",
    "        \"\"\"\n",
    "        Creates an 'as of' static view up to and including the requested 'as of' date provided.\n",
    "        \n",
    "        :param dataset_id: identifier of the dataset\n",
    "        :type: str\n",
    "\n",
    "        :param as_of_date: as of date, will include changesets up to this date/time in the view\n",
    "        :type datetime\n",
    "        \n",
    "        :para destination_type:\n",
    "        :type str\n",
    "        \n",
    "        :param partition_columns: columns to partition the data by for the created view\n",
    "        :type list\n",
    "        \n",
    "        :param sort_columns: column to sort the view by\n",
    "        :type list\n",
    "        \n",
    "        :para destination_properties: \n",
    "        :type str\n",
    "        \n",
    "        :param wait_for_completion: should the function wait for the system to create the view?\n",
    "        :type bool\n",
    "        \n",
    "        :return str: GUID of the created view if successful\n",
    "        \n",
    "        \"\"\"\n",
    "        create_materialized_view_resp = self.client.create_materialized_snapshot(\n",
    "            datasetId=dataset_id, \n",
    "            asOfTimestamp=as_of_date, \n",
    "            destinationType=destination_type, \n",
    "            partitionColumns=partition_columns,\n",
    "            sortColumns=sort_columns,\n",
    "            autoUpdate=False,\n",
    "            destinationProperties=destination_properties\n",
    "        )\n",
    "        view_id = create_materialized_view_resp['id']\n",
    "        if wait_for_completion:\n",
    "            self.wait_for_view(dataset_id=dataset_id, view_id=view_id)\n",
    "        return view_id\n",
    "    \n",
    "    def create_auto_update_view(self, dataset_id: str, destination_type: str, \n",
    "                              partition_columns=[], sort_columns=[], destination_properties={}, wait_for_completion=True):\n",
    "        \"\"\"\n",
    "        Creates an auto-updating view of the given dataset\n",
    "        \n",
    "        :param dataset_id: identifier of the dataset\n",
    "        :type: str\n",
    "\n",
    "        :para destination_type:\n",
    "        :type str\n",
    "        \n",
    "        :param partition_columns: columns to partition the data by for the created view\n",
    "        :type list\n",
    "        \n",
    "        :param sort_columns: column to sort the view by\n",
    "        :type list\n",
    "        \n",
    "        :para destination_properties: \n",
    "        :type str\n",
    "        \n",
    "        :param wait_for_completion: should the function wait for the system to create the view?\n",
    "        :type bool\n",
    "        \n",
    "        :return str: GUID of the created view if successful\n",
    "        \n",
    "        \"\"\"\n",
    "        create_materialized_view_resp = self.client.create_materialized_snapshot(\n",
    "            datasetId=dataset_id,\n",
    "            destinationType=destination_type, \n",
    "            partitionColumns=partition_columns,\n",
    "            sortColumns=sort_columns,\n",
    "            autoUpdate=True,\n",
    "            destinationProperties=destination_properties\n",
    "        )\n",
    "        view_id = create_materialized_view_resp['id']\n",
    "        if wait_for_completion:\n",
    "            self.wait_for_view(dataset_id=dataset_id, view_id=view_id)\n",
    "        return view_id\n",
    "        \n",
    "    def wait_for_ingestion(self, dataset_id: str, changeset_id: str, sleep_sec = 10):\n",
    "        \"\"\"\n",
    "        function that will continuously poll the changeset creation to ensure it completes or fails before returning.\n",
    "        \n",
    "        :param dataset_id: GUID of the dataset\n",
    "        :type: str\n",
    "        \n",
    "        :param changeset_id: GUID of the changeset\n",
    "        :type: str\n",
    "        \n",
    "        :param sleep_sec: seconds to wait between checks\n",
    "        :type: int\n",
    "        \n",
    "        \"\"\"\n",
    "        while True:\n",
    "            status = self.describe_changeset(dataset_id=dataset_id, changeset_id=changeset_id)['status']\n",
    "            if status == 'SUCCESS':\n",
    "                print(f\"Changeset complete\")\n",
    "                break\n",
    "            elif status == 'PENDING' or status == 'RUNNING':\n",
    "                print(f\"Changeset status is still PENDING, waiting {sleep_sec} sec ...\")\n",
    "                time.sleep(sleep_sec)\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Bad changeset status: {status}, failing now.\")\n",
    "                \n",
    "    def wait_for_view(self, dataset_id: str, view_id: str, sleep_sec = 10):\n",
    "        \"\"\"\n",
    "        function that will continuously poll the view creation to ensure it completes or fails before returning.\n",
    "        \n",
    "        :param dataset_id: GUID of the dataset\n",
    "        :type: str\n",
    "        \n",
    "        :param view_id: GUID of the view\n",
    "        :type: str\n",
    "        \n",
    "        :param sleep_sec: seconds to wait between checks\n",
    "        :type: int\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            list_views_resp = self.client.list_materialization_snapshots(datasetId=dataset_id, maxResults=100)\n",
    "            matched_views = list(filter(lambda d: d['id'] == view_id, list_views_resp['materializationSnapshots']))\n",
    "\n",
    "            if len(matched_views) != 1:\n",
    "                size = len(matched_views)\n",
    "                raise Exception(f\"Unexpected error: found {size} views that match the view Id: {view_id}\")\n",
    "\n",
    "            status = matched_views[0]['status']\n",
    "            if status == 'SUCCESS':\n",
    "                print(f\"View complete\")\n",
    "                break\n",
    "            elif status == 'PENDING' or status == 'RUNNING':\n",
    "                print(f\"View status is still PENDING, continue to wait till finish...\")\n",
    "                time.sleep(sleep_sec)\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Bad view status: {status}, failing now.\")\n",
    "\n",
    "    def list_changesets(self, dataset_id: str):\n",
    "        resp = self.client.list_changesets(datasetId=dataset_id, sortKey='CREATE_TIMESTAMP')\n",
    "        results = resp['changesets']\n",
    "\n",
    "        while \"nextToken\" in resp:\n",
    "            resp = self.client.list_changesets(datasetId=dataset_id, sortKey='CREATE_TIMESTAMP', nextToken=resp['nextToken'])\n",
    "            results.extend(resp['changesets'])\n",
    "\n",
    "        return( results )\n",
    "    \n",
    "    def list_views(self, dataset_id: str, max_results = 50):\n",
    "        resp = self.client.list_materialization_snapshots(datasetId=dataset_id, maxResults=max_results)\n",
    "        results = resp['materializationSnapshots']\n",
    "\n",
    "        while \"nextToken\" in resp:\n",
    "            resp = self.client.list_materialization_snapshots(datasetId=dataset_id, maxResults=max_results, nextToken=resp['nextToken'])\n",
    "            results.extend(resp['materializationSnapshots'])\n",
    "\n",
    "        return( results )\n",
    "\n",
    "    def list_datasets(self, maxResults: int):\n",
    "        all_datasets = self.client.list_datasets(maxResults=maxResults)\n",
    "        return ( self.get_list( all_datasets, 'datasets') )\n",
    "    \n",
    "    def list_dataset_types(self):\n",
    "        resp = self.client.list_dataset_types(sort='NAME')\n",
    "        results = resp['datasetTypeSummaries']\n",
    "\n",
    "        while \"nextToken\" in resp:\n",
    "            resp = self.client.list_dataset_types(sort='NAME', nextToken=resp['nextToken'])\n",
    "            results.extend(resp['datasetTypeSummaries'])\n",
    "\n",
    "        return( results )\n",
    "    \n",
    "    def get_execution_role(self):\n",
    "        \"\"\"\n",
    "        Convenience function from SageMaker to get the execution role of the user of the sagemaker studio notebook\n",
    "        \n",
    "        :return: the ARN of the execution role in the sagemaker studio notebook\n",
    "        \"\"\"\n",
    "        import sagemaker as sm\n",
    "        \n",
    "        e_role = sm.get_execution_role()\n",
    "        return ( f\"{e_role}\" )\n",
    "    \n",
    "    def get_user_ingestion_info(self):\n",
    "        return( self.client.get_user_ingestion_info() )\n",
    "\n",
    "    def upload_pandas(self, data_frame: pd.DataFrame):\n",
    "        import awswrangler as wr\n",
    "        resp = self.client.get_user_ingestion_info()\n",
    "        upload_location = resp['ingestionPath']\n",
    "        wr.s3.to_parquet(data_frame, f\"{upload_location}data.parquet\", index=False, boto3_session=self._boto3_session)\n",
    "        return upload_location\n",
    "    \n",
    "    def ingest_pandas(self, data_frame: pd.DataFrame, dataset_id: str, change_type: str, wait_for_completion=True):\n",
    "        print(\"Uploading the pandas dataframe ...\")\n",
    "        upload_location = self.upload_pandas(data_frame)\n",
    "        \n",
    "        print(\"Data upload finished. Ingesting data ...\")\n",
    "        return self.ingest_from_s3(upload_location, dataset_id, change_type, wait_for_completion, format_type='PARQUET')\n",
    "\n",
    "    def read_view_as_pandas(\n",
    "        self,\n",
    "        dataset_id: str,\n",
    "        view_id: str\n",
    "        ):\n",
    "        import awswrangler as wr   # use awswrangler to read the table\n",
    "        \"\"\"\n",
    "        Returns a pandas dataframe the view of the given dataset.  Views in FinSpace can be quite large, be careful!\n",
    "        \n",
    "        :param dataset_id: \n",
    "        :param view_id:\n",
    "        \n",
    "        :return: Pandas dataframe with all data of the view\n",
    "        \"\"\"\n",
    "        \n",
    "        # @todo: switch to DescribeMateriliazation when available in HFS\n",
    "        views = self.list_views(dataset_id=dataset_id, max_results=50)\n",
    "        filtered = [v for v in views if v['id'] == view_id]\n",
    "\n",
    "        if len(filtered) == 0:\n",
    "            raise Exception('No such view found')\n",
    "        if len(filtered) > 1:\n",
    "            raise Exception('Internal Server error')\n",
    "        view = filtered[0]\n",
    "        \n",
    "        # 0. Ensure view is ready to be read\n",
    "        if (view['status'] != 'SUCCESS'): \n",
    "            status = view['status'] \n",
    "            print(f'view run status is not ready: {status}. Returning empty.')\n",
    "            return\n",
    "\n",
    "        glue_db_name = view['destinationTypeProperties']['databaseName']\n",
    "        glue_table_name = view['destinationTypeProperties']['tableName']\n",
    "        \n",
    "        # determine if the table has partitions first, different way to read is there are partitions\n",
    "        p = wr.catalog.get_partitions( table = glue_table_name, database = glue_db_name, boto3_session=self._boto3_session )\n",
    "        df = None\n",
    "        \n",
    "        def no_filter(partitions):\n",
    "            if len(partitions.keys()) > 0: \n",
    "                return True\n",
    "\n",
    "            return False\n",
    "        \n",
    "        if (len(p) == 0):\n",
    "            df = wr.s3.read_parquet_table( table = glue_table_name, database = glue_db_name, boto3_session=self._boto3_session )\n",
    "        else:\n",
    "            spath = wr.catalog.get_table_location( table = glue_table_name, database = glue_db_name, boto3_session=self._boto3_session )\n",
    "            cpath = wr.s3.list_directories(f\"{spath}/*\", boto3_session=self._boto3_session)\n",
    "\n",
    "            read_path = f\"{spath}/\"\n",
    "            \n",
    "            # just one?  Read it\n",
    "            if len(cpath) == 1: \n",
    "                read_path = cpath[0]\n",
    "\n",
    "            df = wr.s3.read_parquet( read_path, dataset=True, partition_filter=no_filter, boto3_session=hab_session )\n",
    "\n",
    "            \n",
    "        # Query Glue table directly with wrangler\n",
    "        return df\n",
    "    \n",
    "    def get_schema_from_pandas(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Returns the FinSpace schema columns from the given pandas dataframe.\n",
    "        \n",
    "        :param df: pandas dataframe to interrogate for the schema\n",
    "        :type pd.DataFrame:\n",
    "        \n",
    "        :return: FinSpace column schema list\n",
    "        \"\"\"\n",
    "        \n",
    "        # for translation to FinSpace's schema\n",
    "        # 'STRING'|'CHAR'|'INTEGER'|'TINYINT'|'SMALLINT'|'BIGINT'|'FLOAT'|'DOUBLE'|'DATE'|'DATETIME'|'BOOLEAN'|'BINARY'\n",
    "        DoubleType    = \"DOUBLE\"\n",
    "        FloatType     = \"FLOAT\"\n",
    "        DateType      = \"DATE\"\n",
    "        StringType    = \"STRING\"\n",
    "        IntegerType   = \"INTEGER\"\n",
    "        LongType      = \"BIGINT\"\n",
    "        BooleanType   = \"BOOLEAN\"\n",
    "        TimestampType = \"DATETIME\"\n",
    "\n",
    "        hab_columns = []\n",
    "        \n",
    "        for name in dict(df.dtypes):\n",
    "\n",
    "            p_type = df.dtypes[name]\n",
    "\n",
    "            switcher = {\n",
    "                \"float64\"             : DoubleType,\n",
    "                \"int64\"               : IntegerType,\n",
    "                \"datetime64[ns, UTC]\" : TimestampType,\n",
    "                \"datetime64[ns]\"      : DateType\n",
    "            }\n",
    "\n",
    "            habType = switcher.get( str(p_type), StringType)\n",
    "\n",
    "            hab_columns.append({\n",
    "                \"dataType\"    : habType, \n",
    "                \"name\"        : name,\n",
    "                \"description\" : \"\"\n",
    "            })\n",
    "\n",
    "        return( hab_columns )\n",
    "\n",
    "    def get_date_cols(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Returns which are the data columns found in the pandas dataframe. \n",
    "        Pandas does the hard work to figure out which of the columns can be considered to be date columns.\n",
    "        \n",
    "        :param df: pandas dataframe to interrogate for the schema\n",
    "        :type pd.DataFrame:\n",
    "        \n",
    "        :return: list of column names that can be parsed as dates by pandas\n",
    "\n",
    "        \"\"\"\n",
    "        dateCols = []\n",
    "\n",
    "        for name in dict(df.dtypes):\n",
    "\n",
    "            p_type = df.dtypes[name]\n",
    "\n",
    "            switcher = {\n",
    "                \"datetime64[ns, UTC]\" : True,\n",
    "                \"datetime64[ns]\"      : True\n",
    "            }\n",
    "\n",
    "            if str(p_type).startswith(\"date\"):\n",
    "                dateCols.append(name)\n",
    "\n",
    "        return( dateCols )\n",
    "        \n",
    "    def get_best_schema_from_csv(self, path, is_s3 = True, read_rows=500, sep=','):\n",
    "        \"\"\"\n",
    "        Uses multiple reads of the file with pandas to determine schema of the referenced files. Files are expected to be csv.\n",
    "        \n",
    "        :param path: path to the files to read\n",
    "        :type str\n",
    "        \n",
    "        :param is_s3: True if the path is s3;  False if filesystem\n",
    "        :type bool\n",
    "        \n",
    "        :return dict: schema for FinSpace\n",
    "        \"\"\"\n",
    "        #\n",
    "        # best efforts to determine the schema, sight unseen\n",
    "        import awswrangler as wr\n",
    "\n",
    "        schema_rows = 500\n",
    "\n",
    "        # 1: get the base schema\n",
    "        df1 = None\n",
    "\n",
    "        if is_s3:\n",
    "            df1 = wr.s3.read_csv(path, nrows=schema_rows, sep=sep) \n",
    "        else:\n",
    "            df1 = pd.read_csv(path, nrows=schema_rows, sep=sep)\n",
    "\n",
    "        num_cols = len(df1.columns)\n",
    "\n",
    "        # with number of columns, try to infer dates\n",
    "        df2 = None\n",
    "        \n",
    "        if is_s3:\n",
    "            df2 = wr.s3.read_csv(path, parse_dates=list(range(0,num_cols)), infer_datetime_format=True, nrows=schema_rows, sep=sep)\n",
    "        else:\n",
    "            df2 = pd.read_csv(path, parse_dates=list(range(0,num_cols)), infer_datetime_format=True, nrows=schema_rows, sep=sep)\n",
    "\n",
    "        date_cols = self.get_date_cols(df2)\n",
    "\n",
    "        # with dates known, parse the file fully\n",
    "        df = None\n",
    "        \n",
    "        if is_s3:\n",
    "            df = wr.s3.read_csv(path, parse_dates=date_cols, infer_datetime_format=True, nrows=schema_rows, sep=sep)\n",
    "        else:\n",
    "            df = pd.read_csv(path, parse_dates=date_cols, infer_datetime_format=True, nrows=schema_rows, sep=sep)\n",
    "        \n",
    "        schema_cols = self.get_schema_from_pandas(df)\n",
    "        \n",
    "        return( schema_cols )\n",
    "\n",
    "    def s3_upload_file(self, sourceFile: str, s3_destination: str):\n",
    "        \"\"\"\n",
    "        Uploads a local file (full path) to the s3 destination given (expected form: s3://<bucket>/<prefix>/). The filename will have spaces replaced with _.\n",
    "        \n",
    "        :param s3_destination: full path to where to save the file\n",
    "        :type str\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        hab_s3_client = self._boto3_session.client(service_name = 's3')\n",
    "        \n",
    "        o = urlparse(s3_destination)\n",
    "        bucket = o.netloc\n",
    "        prefix = o.path.lstrip('/')\n",
    "\n",
    "        fname = os.path.basename(sourceFile)\n",
    "\n",
    "        hab_s3_client.upload_file(sourceFile, bucket, f\"{prefix}{fname.replace(' ', '_')}\" )\n",
    "        \n",
    "    def list_objects( self, s3_location: str ):\n",
    "        \"\"\"\n",
    "        lists the objects found at the s3_location. Strips out the boto API response header, just returns the contents of the location. Internally uses the list_objects_v2.\n",
    "        \n",
    "        :param s3_location: path, starting with s3:// to get the list of objects from\n",
    "        :type str\n",
    "                \n",
    "        \"\"\"\n",
    "        o = urlparse(s3_location)\n",
    "        bucket = o.netloc\n",
    "        prefix = o.path.lstrip('/')\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        hab_s3_client = self._boto3_session.client(service_name = 's3')\n",
    "\n",
    "        paginator = hab_s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                results.extend(page['Contents'])\n",
    "                \n",
    "        return ( results )\n",
    "\n",
    "    def list_clusters( self, status: str = None): \n",
    "        \"\"\"\n",
    "        Lists current clusters and their statuses\n",
    "        \n",
    "        :param status: status to filter for\n",
    "        \n",
    "        :return dict: list of clusters\n",
    "        \"\"\"\n",
    "        \n",
    "        resp = self.client.list_clusters()\n",
    "\n",
    "        clusters = []\n",
    "        \n",
    "        if 'clusters' not in resp:\n",
    "            return( clusters )\n",
    "\n",
    "        for c in resp['clusters']:\n",
    "            if status is None:\n",
    "                clusters.append(c)\n",
    "            else:\n",
    "                if c['clusterStatus']['state'] in status: \n",
    "                    clusters.append(c)\n",
    "\n",
    "        return( clusters )\n",
    "    \n",
    "    def get_cluster( self, clusterId ):\n",
    "        \"\"\"\n",
    "        Resize the given cluster to desired template\n",
    "        \n",
    "        :param clusterId: cluster id\n",
    "        :param template: target template to resize to\n",
    "        \"\"\"\n",
    "\n",
    "        clusters = self.list_clusters()\n",
    "        \n",
    "        for c in clusters:\n",
    "            if c['clusterId'] == clusterId:\n",
    "                return( c )\n",
    "            \n",
    "        return( None )\n",
    "            \n",
    "    def update_cluster( self, clusterId:str, template:str ): \n",
    "        \"\"\"\n",
    "        Resize the given cluster to desired template\n",
    "        \n",
    "        :param clusterId: cluster id\n",
    "        :param template: target template to resize to\n",
    "        \"\"\"\n",
    "        \n",
    "        cluster = self.get_cluster(clusterId = clusterId)\n",
    "        \n",
    "        if cluster['currentTemplate'] == template:\n",
    "            print(f\"Already using template: {template}\")\n",
    "            return( cluster )\n",
    "        \n",
    "        self.client.update_cluster(clusterId=clusterId, template=template)\n",
    "        \n",
    "        return( self.get_cluster(clusterId = clusterId) )\n",
    "        \n",
    "        \n",
    "    def wait_for_status( self, clusterId:str, status:str, sleep_sec = 10, max_wait_sec = 900 ):\n",
    "        \"\"\"\n",
    "        Function polls service until cluster is in desired status.\n",
    "        \n",
    "        :param clusterId: the cluster's ID\n",
    "        :param status: desired status for clsuter to reach\n",
    "        :\n",
    "        \"\"\"\n",
    "        total_wait = 0\n",
    "        \n",
    "        while True and total_wait < max_wait_sec:\n",
    "            resp = self.client.list_clusters()\n",
    "\n",
    "            this_cluster = None\n",
    "            \n",
    "            # is this the cluster?\n",
    "            for c in resp['clusters']:\n",
    "                if clusterId == c['clusterId']:\n",
    "                    this_cluster = c\n",
    "\n",
    "            if this_cluster is None:\n",
    "                print(f\"clusterId:{clusterId} not found\")\n",
    "                return( None )\n",
    "            \n",
    "            this_status = this_cluster['clusterStatus']['state']\n",
    "                    \n",
    "            if this_status.upper() != status.upper():\n",
    "                print(f\"Cluster status is {this_status}, waiting {sleep_sec} sec ...\")\n",
    "                time.sleep(sleep_sec)\n",
    "                total_wait = total_wait + sleep_sec\n",
    "                continue\n",
    "            else:\n",
    "                return( this_cluster )\n",
    "            \n",
    "    def get_working_location(self, locationType='SAGEMAKER'):\n",
    "        resp = None\n",
    "        location = self.client.get_working_location(locationType=locationType)\n",
    "        \n",
    "        if 's3Uri' in location:\n",
    "            resp = location['s3Uri']\n",
    "        \n",
    "        return( resp )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Python Helper Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hab_session = boto3.session.Session(\n",
    "    region_name           = region_name,\n",
    "    aws_access_key_id     = hab_access_key_id,\n",
    "    aws_secret_access_key = hab_secret_access_key,\n",
    "    aws_session_token     = hab_session_token\n",
    ")\n",
    "\n",
    "finspace = FinSpace(\n",
    "    boto_session = hab_session,\n",
    "    dev_overrides = {\n",
    "        \"region_name\" : region_name \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Management\n",
    "\n",
    "functions available fro the fiunspace client:\n",
    "- list_clusters\n",
    "- create_cluster\n",
    "- update_cluster\n",
    "- terminate_cluster\n",
    "- get_cluster_connection_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = finspace.list_clusters()\n",
    "pd.DataFrame.from_dict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the running cluster and resize it\n",
    "active_clusters = finspace.list_clusters( ['RUNNING', 'UPDATING'] )\n",
    "pd.DataFrame.from_dict(active_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(active_clusters) > 0:\n",
    "    cid = active_clusters[0]['clusterId']\n",
    "\n",
    "    this_cluster = finspace.get_cluster( clusterId=cid )\n",
    "    print( pd.DataFrame.from_dict(this_cluster) )\n",
    "else:\n",
    "    print(\"no clusters running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finspace.wait_for_status( clusterId = cid, status = 'RUNNING' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change a cluster's size, call update\n",
    "finspace.update_cluster(clusterId = cid, template = 'Medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finspace.wait_for_status( clusterId = cid, status = 'RUNNING' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
