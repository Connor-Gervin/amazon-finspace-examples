{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "from aws.finspace.cluster import FinSpaceClusterManager\n",
    "\n",
    "# if this was already run, no need to run again\n",
    "if 'finspace_clusters' not in globals():\n",
    "    finspace_clusters = FinSpaceClusterManager()\n",
    "    finspace_clusters.auto_connect()\n",
    "else:\n",
    "    print(f'connected to cluster: {finspace_clusters.get_connected_cluster_id()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Timebars and Summarize\n",
    "Time bars are obtained by sampling information at fixed time intervals, e.g., once every minute. \n",
    "\n",
    "**Series:** Time Series Data Engineering and Analysis\n",
    "\n",
    "As part of the big data timeseries processing workflow Habanero supports, show how one takes raw, uneven in time events of TAQ data and collects them into a performant derived dataset of collected bars of data.\n",
    "\n",
    "\n",
    "### Timeseries Workflow\n",
    "Raw Events → **\\[Collect bars → Summarize bars\\]** → Fill Missing → Prepare → Analytics\n",
    "\n",
    "This is the collect bars stage of time series data workflow, where the raw and randomly arranged event data is collected into even bars for future summary.\n",
    "\n",
    "![Workflow](workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH CORRECT IDS!\n",
    "# US Equity TAQ Sample - AMZN 6 Months - Sample\n",
    "source_dataset_id = ''\n",
    "source_view_id    = ''\n",
    "\n",
    "# Group: Analyst\n",
    "basicPermissionGroupId = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook imports\n",
    "import time\n",
    "import datetime as dt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pprint \n",
    "\n",
    "# Habanero imports\n",
    "from aws.finspace.timeseries.spark.util import string_to_timestamp_micros\n",
    "from aws.finspace.timeseries.spark.windows import create_time_bars, compute_analytics_on_features, compute_features_on_time_bars\n",
    "from aws.finspace.timeseries.spark.spec import BarInputSpec, TimeBarSpec\n",
    "from aws.finspace.timeseries.spark.summarizer import *\n",
    "\n",
    "# destination if adding to an existing dataset\n",
    "dest_dataset_id   = None\n",
    "\n",
    "start_date = \"2019-10-01\"\n",
    "end_date   = \"2019-12-31\"\n",
    "\n",
    "barNum  = 3\n",
    "barUnit = \"minute\"\n",
    "\n",
    "# \n",
    "d = time.strftime('%Y-%m-%d %-I:%M %p %Z')  # name is unique to date and time created\n",
    "\n",
    "name = f\"TAQ Timebar Summaries - DEMO ({barNum} {barUnit})\"\n",
    "description = f\"TAQ data summarized into time bars of {barNum} {barUnit} containing STD, VWAP, OHLC and Total Volume. start: {start_date} end: {end_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permissions that will be given on the dataset\n",
    "basicPermissions = [\n",
    "    \"ViewDatasetDetails\" \n",
    "    ,\"ReadDatasetData\" \n",
    "    ,\"AddDatasetData\" \n",
    "    ,\"CreateSnapshot\" \n",
    "    ,\"EditDatasetMetadata\"\n",
    "    ,\"ManageDatasetPermissions\"\n",
    "    ,\"DeleteDataset\"\n",
    "]\n",
    "\n",
    "basicOwnerInfo = {\n",
    "    \"phoneNumber\" : \"12125551000\",\n",
    "    \"email\"       : \"jdoe@amazon.com\",\n",
    "    \"name\"        : \"John Doe\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a series of dates from a given start/stop date\n",
    "def daterange(startD, endD):\n",
    "    for n in range(int ((endD - startD).days)+1):\n",
    "        yield startD + dt.timedelta(n)\n",
    "\n",
    "#\n",
    "def businessDatesBetween(startD, endD):\n",
    "    weekdays = [6, 7]\n",
    "\n",
    "    holidays = [ dt.date(2019, 11, 28), \n",
    "             dt.date(2019, 12, 25), \n",
    "             dt.date(2020, 1, 1), \n",
    "             dt.date(2020, 1, 20), \n",
    "             dt.date(2020, 2, 17),\n",
    "             dt.date(2020, 4, 10),\n",
    "             dt.date(2020, 5, 25),\n",
    "             dt.date(2020, 7, 3), \n",
    "             dt.date(2020, 9, 7),\n",
    "             dt.date(2020, 11, 26) ]\n",
    "\n",
    "    processDates = list()\n",
    "\n",
    "    for aDate in daterange(startD, endD):\n",
    "        if (aDate.isoweekday() not in weekdays) & (aDate not in holidays):                    \n",
    "            processDates.append( aDate )\n",
    "    \n",
    "    return( processDates )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../Utilities/finspace.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../Utilities/finspace_spark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Connect\n",
    "finspace = SparkFinSpace( spark = spark )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data from FinSpace\n",
    "Using the given dataset and view ids, get the view as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finspace.list_classifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finspace.describe_dataset_details(dataset_id = source_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finspace.list_views(dataset_id = source_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDF = finspace.read_view_as_spark(dataset_id = source_dataset_id, view_id = source_view_id)\n",
    "tDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with the DataFrame\n",
    "As a Spark DataFrame, you can interact with the data using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS: Collect and Summarize\n",
    "The functions below process the time series data by first collecting the data into time-bars then summarizing the data captured in the bar. The bars are collected into a column 'activity' for the window of time in the collectTimeBars function. The summarize bar function's purpose is to summarize the data collected in the bar, that bar can be of any type, not just time.\n",
    "\n",
    "Customizations\n",
    "- vary the width and steps of the time-bar, collect different data from the source DataFrame\n",
    "- Summarize the bar with other calculations  \n",
    "\n",
    "Bring Your Own  \n",
    "- Customers can add their own custom Spark user defined functions (UDF) into the summarizer phase\n",
    "\n",
    "![Workflow](workflow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------\n",
    "# Collects event data into Time-Bars\n",
    "#\n",
    "# barWidth: number and units and time, e.g. '1 minute'\n",
    "#-------------------------------------------------------------------\n",
    "def collectTimeBars( taqDF, barWidth ): \n",
    "\n",
    "    # define the time-bar, column for time and how much time to collect\n",
    "    timebar_spec   = TimeBarSpec(timestamp_column='datetime', window_duration=barWidth, slide_duration=barWidth)\n",
    "    \n",
    "    # what from the source DataFrame to collect in the bar\n",
    "    bar_input_spec = BarInputSpec('activity', 'datetime', 'timestamp', 'price', 'quantity', 'exchange', 'conditions' )\n",
    "\n",
    "    # The results in a new DataFrame\n",
    "    barDF = ( create_time_bars(data=taqDF, \n",
    "                             timebar_column='window', \n",
    "                             grouping_col_list=['date', 'ticker', 'eventtype'], \n",
    "                             input_spec=bar_input_spec, \n",
    "                             timebar_spec=timebar_spec)\n",
    "        .withColumn('activity_count', F.size(F.col('activity'))) )\n",
    "\n",
    "    return( barDF )\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Summarizes the data that was collected in the bar\n",
    "#-------------------------------------------------------------------\n",
    "def summarizeBars( barDF ):\n",
    "\n",
    "# Bar data is in a column that is a list of structs named 'activity'\n",
    "# values collected in 'activity': datetime, teimstamp, price, quantity, exchange, conditions\n",
    "    \n",
    "    sumDF = ( barDF\n",
    "        .withColumn( 'std',    std( 'activity.price' ) )\n",
    "        .withColumn( 'vwap',   vwap( 'activity.price', 'activity.quantity' ) )\n",
    "        .withColumn( 'ohlc',   ohlc_func( 'activity.datetime', 'activity.price' ) ) \n",
    "        .withColumn( 'volume', total_volume( 'activity.quantity' ) )\n",
    "#        .withColumn('MY_RESULT', MY_SPECIAL_FUNCTION( 'activity.datetime', 'activity.price', 'activity.quantity' ) )\n",
    "        .drop( barDF.activity )\n",
    "    )\n",
    "\n",
    "    return( sumDF )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Spark DataFrame\n",
    "Create a Spark dataframe that is the result of the data pipline to collect TAQ data into time bars and then summarizes each bar.\n",
    "\n",
    "## Outline of Processing\n",
    "- for each set of dates in the overall range....\n",
    "- collect data into time bars\n",
    "- summarize the data for each bar\n",
    "- save as a changeset to the dataset\n",
    "  - creates a new dataset if one does not exist yet\n",
    "  - uses the habanero APIs to simpliffy dataset creation from a Spark DataFrame\n",
    "- continue until all dates have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to dates\n",
    "start_dt = dt.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end_dt   = dt.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "# get the list of business dates between given dates\n",
    "processDates = businessDatesBetween( start_dt, end_dt )\n",
    "\n",
    "# grabs a set items from the list, allows us to iterate with a set of dates at a time\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "chunk_size    = 3\n",
    "barPartitions = None\n",
    "sumPartitions = 4\n",
    "partitionCol  = \"date\"\n",
    "\n",
    "# necessary for time bar API\n",
    "barWidth = f\"{barNum} {barUnit}\"\n",
    "\n",
    "isFirst = True\n",
    "\n",
    "for dates in chunker(processDates, chunk_size):\n",
    "    print(f\"Processing {len(dates)}: {dates}\")\n",
    "\n",
    "    # filter the data for the day\n",
    "    dayDF = tDF.filter( tDF.date.isin(dates))\n",
    "\n",
    "    # collect the data into time bars of the desired width\n",
    "    dayDF = collectTimeBars( dayDF, barWidth )\n",
    "\n",
    "    # summarize the bars, drop activity since its no longer needed\n",
    "    dayDF = summarizeBars( dayDF ).drop('activity')\n",
    "\n",
    "    # add indicators using summaries\n",
    "    #dayDF = addIndicators( dayDF, numSteps = 10, shortStep = 12, longStep = 26)\n",
    "\n",
    "    ## flatted the complex schema into a simple one, drop columns no longer needed\n",
    "    finalDF = ( dayDF\n",
    "        .withColumn(\"start\", dayDF.window.start)\n",
    "        .withColumn(\"end\",   dayDF.window.end)\n",
    "\n",
    "        .withColumn(\"open\",  dayDF.ohlc.open)\n",
    "        .withColumn(\"high\",  dayDF.ohlc.high)\n",
    "        .withColumn(\"low\",   dayDF.ohlc.low)\n",
    "        .withColumn(\"close\", dayDF.ohlc.close)\n",
    "\n",
    "        .drop(\"window\")\n",
    "        .drop(\"ohlc\")\n",
    "    )\n",
    "    \n",
    "    # create the changeset\n",
    "    change_type = \"APPEND\"\n",
    "    \n",
    "    # is this the first pass and no dest_dateset_id given, create the dataset\n",
    "    if (isFirst and dest_dataset_id is None): \n",
    "        \n",
    "        # Get schema from the DataFrame\n",
    "        schema =  {\n",
    "            \"columns\": finspace.get_schema_from_spark(finalDF),\n",
    "            \"primaryKeyColumns\": [ ]  \n",
    "        }\n",
    "\n",
    "        print(\"creating dataset\")\n",
    "        pprint.pprint(schema)\n",
    "\n",
    "        # Create the dataset if it does not exist yet\n",
    "        dest_dataset_id = finspace.create_dataset(\n",
    "            name = name, \n",
    "            description = description, \n",
    "            permission_group_id = basicPermissionGroupId,\n",
    "            dataset_permissions = basicPermissions,\n",
    "            kind = \"TABULAR\",\n",
    "            owner_info = basicOwnerInfo,\n",
    "            schema = schema\n",
    "        )\n",
    "\n",
    "        # first changeset will be a replace\n",
    "        change_type = \"REPLACE\"\n",
    "\n",
    "        print( f\"Created dest_dataset_id= {dest_dataset_id}\")        \n",
    "\n",
    "    print(f\"Creating Changeset: {change_type}\")\n",
    "    changeset_id = finspace.ingest_dataframe(data_frame=finalDF, dataset_id = dest_dataset_id, change_type=change_type, wait_for_completion=True)\n",
    "    \n",
    "    isFirst = False\n",
    "    \n",
    "    print(f\"changeset_id = {changeset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Views of the Dataset\n",
    "use the habanero APIs to create 2 views of the data, an 'as-of' view for state up to this moment, and an additional auto-updating view if one does not exist for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"dest_dataset_id= {dest_dataset_id}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_views = finspace.list_views(dataset_id = dest_dataset_id, max_results=100)\n",
    "\n",
    "autoupdate_view_id = None\n",
    "\n",
    "for ss in existing_views:\n",
    "    if ss['autoUpdate'] == True: \n",
    "        autoupdate_view_id = ss['id']\n",
    "        \n",
    "autoupdate_view_id        \n",
    "\n",
    "# create a an auto-update snapshot for this dataset if one does not already exist\n",
    "if (autoupdate_view_id is None):\n",
    "    print(\"creating auto-update view\")\n",
    "\n",
    "    autoupdate_view_id = finspace.create_auto_update_view(\n",
    "        dataset_id = dest_dataset_id, \n",
    "        destination_type = \"GLUE_TABLE\",\n",
    "        partition_columns = [\"date\"], \n",
    "        sort_columns = [\"end\"], \n",
    "        wait_for_completion = True)\n",
    "else:\n",
    "    print(f\"Exists: autoupdate_view_id = {autoupdate_view_id}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset_id = '{dest_dataset_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
